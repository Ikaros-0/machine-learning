# knn学习
## 一、初始knn
k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart P提出的一种基本分类与回归方法。

它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。

一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。

最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类

## 二、knn的距离度量
以特征数据量为维度的欧式距离:  

<img src="./imgs/knn-4.png"><br>

即：假设特征数据量n=2，则距离为一下公式：   

<img src="./imgs/knn-1.png"><br>
  
knn算法步骤如下：
<ol>
    <li>计算已知类别数据集中的点与当前点之间的距离；</li>
    <li>按照距离递增次序排序；</li>
    <li>选取与当前点距离最小的k个点；</li>
    <li>确定前k个点所在类别的出现频率；</li>
    <li>返回前k个点所出现频率最高的类别作为当前点的预测分类。</li>
</ol><br>

### eg:爱情/动作片:
<br>
<img src="./imgs/knn-2.png"><br><br>
<img src="./imgs/knn-3.png"><br><br>
编程实现“爱情/动作片”:<br>
详情见文件：./codes/film_1.py 和 ./codes/film_2.py  

### knn分类器评估：
通过大量的测试数据，我们可以得到分类器的错误率，即分类器给出错误结果的次数除以测试执行的总数。

错误率是常用的评估方法，主要用于评估分类器在某个数据集上的执行效果。完美分类器的错误率为0，最差分类器的错误率是1.0。

同时，我们也不难发现，k-近邻算法没有进行数据的训练，直接使用未知的数据与已知的数据进行比较，得到结果。因此，可以说k-近邻算法不具有显式的学习过程。
## 二、约会网站配对效果判定
k-近邻算法的一般流程：
<ol>
    <li>收集数据：可以使用爬虫进行数据的收集，也可以使用第三方提供的免费或收费的数据。一般来讲，数据放在txt文本文件中，按照一定的格式进行存储，便于解析及处理。</li>
    <li>准备数据：使用Python解析、预处理数据。</li>
    <li>分析数据：可以使用很多方法对数据进行分析，例如使用Matplotlib将数据可视化。</li>
    <li>测试算法：计算错误率</li>
    <li>使用算法：错误率在可接受范围内，就可以运行k-近邻算法进行分类。</li>
</ol>